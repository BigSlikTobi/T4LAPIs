#!/usr/bin/env python3
"""
Test integration of story grouping configuration with existing scripts.
This demonstrates how the new configuration system works with the existing pipeline.
"""

import os
import sys
import tempfile
import yaml
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

def test_config_integration():
    """Test that our configuration integrates well with existing scripts."""
    print("üß™ Testing Story Grouping Configuration Integration")
    print("=" * 60)
    
    # Create a test configuration file
    test_config = {
        "llm": {
            "provider": "openai",
            "model": "gpt-4o-mini",
            "api_key_env": "OPENAI_API_KEY",
            "max_tokens": 500,
            "temperature": 0.1,
            "timeout_seconds": 30,
            "max_retries": 3
        },
        "embedding": {
            "model_name": "all-MiniLM-L6-v2",
            "dimension": 384,
            "batch_size": 16,  # Different from default to test override
            "cache_ttl_hours": 24,
            "normalize_vectors": True
        },
        "similarity": {
            "threshold": 0.75,  # Different from default
            "metric": "cosine",
            "max_candidates": 50,  # Different from default
            "search_limit": 1000,
            "candidate_similarity_floor": 0.4  # Different from default
        },
        "grouping": {
            "max_group_size": 25,  # Different from default
            "centroid_update_threshold": 5,
            "status_transition_hours": 24,
            "auto_tagging": True,
            "max_stories_per_run": None,
            "prioritize_recent": True,
            "prioritize_high_relevance": True,
            "reprocess_existing": False
        },
        "performance": {
            "parallel_processing": True,
            "max_workers": 2,  # Different from default
            "max_parallelism": 2,  # Different from default
            "batch_size": 10,
            "cache_size": 1000,
            "max_total_processing_time": None
        },
        "database": {
            "connection_timeout_seconds": 30,
            "max_connections": 10,
            "batch_insert_size": 100,
            "vector_index_maintenance": True
        },
        "monitoring": {
            "log_level": "DEBUG",  # Different from default
            "metrics_enabled": True,
            "cost_tracking_enabled": True,
            "performance_alerts_enabled": True,
            "group_quality_monitoring": True
        }
    }
    
    # Write test config to temporary file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
        yaml.dump(test_config, f, default_flow_style=False)
        test_config_path = f.name
    
    try:
        # Test 1: Load configuration using our manager
        print("üîß Testing configuration loading...")
        try:
            from nfl_news_pipeline.story_grouping_config import StoryGroupingConfigManager
            
            manager = StoryGroupingConfigManager(test_config_path)
            config = manager.load_config()
            
            print("‚úÖ Configuration loaded successfully")
            
            # Verify some key values were loaded correctly
            assert config.embedding.batch_size == 16, f"Expected batch_size=16, got {config.embedding.batch_size}"
            assert config.similarity.threshold == 0.75, f"Expected threshold=0.75, got {config.similarity.threshold}"
            assert config.performance.max_parallelism == 2, f"Expected max_parallelism=2, got {config.performance.max_parallelism}"
            assert config.monitoring.log_level == "DEBUG", f"Expected log_level=DEBUG, got {config.monitoring.log_level}"
            
            print("‚úÖ Configuration values verified")
            
        except Exception as e:
            print(f"‚ùå Configuration loading failed: {e}")
            return False
        
        # Test 2: Test orchestrator settings conversion
        print("\nüéØ Testing orchestrator settings conversion...")
        try:
            settings = config.get_orchestrator_settings()
            
            # Verify conversion worked
            assert settings.max_parallelism == 2, f"Expected max_parallelism=2, got {settings.max_parallelism}"
            assert settings.max_candidates == 50, f"Expected max_candidates=50, got {settings.max_candidates}"
            assert settings.candidate_similarity_floor == 0.4, f"Expected candidate_similarity_floor=0.4, got {settings.candidate_similarity_floor}"
            
            print("‚úÖ Orchestrator settings converted successfully")
            
        except Exception as e:
            print(f"‚ùå Orchestrator settings conversion failed: {e}")
            return False
        
        # Test 3: Test dry run script argument parsing with config
        print("\nüöÄ Testing dry run script integration...")
        try:
            # Import the argument parser from the dry run script
            sys.path.insert(0, str(PROJECT_ROOT / "scripts"))
            
            # We can't import the full module due to dependencies, but we can test the concept
            # This demonstrates how it would work in practice
            
            # Simulate what the dry run script would do
            mock_args = type('Args', (), {
                'config': test_config_path,
                'provider': 'openai',  # default
                'similarity_threshold': 0.8,  # default
                'sentence_model': 'all-MiniLM-L6-v2',  # default
                'embedding_batch_size': 32,  # default
                'max_parallelism': 4,  # default
                'max_candidates': 8,  # default
                'candidate_floor': 0.35,  # default
                'max_group_size': 50,  # default
                'log_level': 'INFO'  # default
            })()
            
            # Load config and override defaults (simulating what the script does)
            if hasattr(mock_args, 'config') and mock_args.config:
                manager = StoryGroupingConfigManager(mock_args.config)
                config = manager.load_config()
                
                # Override args with config values
                if mock_args.similarity_threshold == 0.8:
                    mock_args.similarity_threshold = config.similarity.threshold
                if mock_args.embedding_batch_size == 32:
                    mock_args.embedding_batch_size = config.embedding.batch_size
                if mock_args.max_parallelism == 4:
                    mock_args.max_parallelism = config.performance.max_parallelism
                if mock_args.max_candidates == 8:
                    mock_args.max_candidates = config.similarity.max_candidates
                if mock_args.candidate_floor == 0.35:
                    mock_args.candidate_floor = config.similarity.candidate_similarity_floor
                if mock_args.max_group_size == 50:
                    mock_args.max_group_size = config.grouping.max_group_size
                if mock_args.log_level == 'INFO':
                    mock_args.log_level = config.monitoring.log_level
                
                # Verify overrides worked
                assert mock_args.similarity_threshold == 0.75, f"Expected threshold=0.75, got {mock_args.similarity_threshold}"
                assert mock_args.embedding_batch_size == 16, f"Expected batch_size=16, got {mock_args.embedding_batch_size}"
                assert mock_args.max_parallelism == 2, f"Expected max_parallelism=2, got {mock_args.max_parallelism}"
                assert mock_args.log_level == "DEBUG", f"Expected log_level=DEBUG, got {mock_args.log_level}"
                
                print("‚úÖ Dry run script integration working correctly")
            
        except Exception as e:
            print(f"‚ùå Dry run script integration failed: {e}")
            return False
        
        # Test 4: Environment variable validation
        print("\nüåç Testing environment validation...")
        try:
            issues = manager.validate_environment()
            if issues:
                print("‚ö†Ô∏è  Environment issues found (expected in test):")
                for issue in issues:
                    print(f"   - {issue}")
            else:
                print("‚úÖ Environment validation passed")
            
        except Exception as e:
            print(f"‚ùå Environment validation failed: {e}")
            return False
        
        print("\n" + "=" * 60)
        print("üéâ All integration tests passed!")
        print("\nüìã Integration Summary:")
        print(f"   Configuration File: {Path(test_config_path).name}")
        print(f"   LLM Provider: {config.llm.provider}")
        print(f"   Embedding Model: {config.embedding.model_name}")
        print(f"   Similarity Threshold: {config.similarity.threshold}")
        print(f"   Max Parallelism: {config.performance.max_parallelism}")
        print(f"   Batch Size: {config.embedding.batch_size}")
        print(f"   Log Level: {config.monitoring.log_level}")
        
        return True
        
    finally:
        # Clean up temporary file
        if os.path.exists(test_config_path):
            os.unlink(test_config_path)

def main():
    """Run integration tests."""
    success = test_config_integration()
    if success:
        print("\n‚úÖ Configuration integration is working correctly!")
        print("\nüöÄ Usage Examples:")
        print("   # Use configuration file with dry run script:")
        print("   python scripts/story_grouping_dry_run.py --config story_grouping_config.yaml")
        print("   ")
        print("   # Override specific parameters:")
        print("   python scripts/story_grouping_dry_run.py --config story_grouping_config.yaml --max-parallelism 8")
        print("   ")
        print("   # Validate deployment:")
        print("   python scripts/deploy_story_grouping.py validate")
        return 0
    else:
        print("\n‚ùå Configuration integration has issues")
        return 1

if __name__ == "__main__":
    sys.exit(main())